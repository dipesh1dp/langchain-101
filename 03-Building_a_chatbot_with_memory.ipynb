{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7fced668-bce6-4a40-a6f0-dcd0956aef58",
   "metadata": {},
   "source": [
    "# 03. Building a Chatbot with Memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c6cffaf-f677-4536-a9f1-6da38f7be80f",
   "metadata": {},
   "source": [
    "## Setting up"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "234ea39e-ff15-40a7-b3c8-08d31647c15a",
   "metadata": {},
   "source": [
    "### LangSmith Tracing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7803ed38-9785-4095-9625-ea409e87ce83",
   "metadata": {},
   "source": [
    "After signing up at [LangSmith](https://smith.langchain.com/), make sure to set your environment variable to start logging traces: \n",
    "```bash \n",
    "export LANGSMITH_TRACING=\"true\"\n",
    "export LANGSMITH_API_KEY=\"...\" \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08fb574e-01da-4b0f-a514-5e9741d64130",
   "metadata": {},
   "source": [
    "or, you can set them manually in a notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2c2aab94-2295-4a57-b1e2-96740a400319",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " ········\n"
     ]
    }
   ],
   "source": [
    "import getpass \n",
    "import os\n",
    "\n",
    "os.environ[\"LANGSMITH_TRACING\"] = \"true\" \n",
    "os.environ[\"LANGSMITH_API_KEY\"] = getpass.getpass()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "678fa208-5358-47b8-8993-79adde3a3d4a",
   "metadata": {},
   "source": [
    "### Get `.env` variable for LLM API Key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "61a1d233-cbc1-4568-88cd-449a8ca8f936",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv() \n",
    "\n",
    "gemini_api_key = os.getenv(\"GEMINI_API_KEY\")\n",
    "\n",
    "if gemini_api_key is None:\n",
    "    raise ValueError(\"GEMINI_API_KEY not found. Please set it up in the .env file\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "136198cf-4514-44fb-bb09-a3baadceea7e",
   "metadata": {},
   "source": [
    "### Initialize the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b191274d-419b-4ba5-9935-f020eac91208",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI \n",
    "\n",
    "model = ChatGoogleGenerativeAI(\n",
    "    model = \"gemini-2.5-flash\", \n",
    "    temperature = 0, \n",
    "    api_key = gemini_api_key\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5afff283-4aa6-4850-8074-6e9b9edf7c0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Hi Dipesh! Nice to meet you.\\n\\nI'm an AI assistant. How can I help you today?\", additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': []}, id='run--8acfad19-f5ee-46e1-8056-6a92fdba00fa-0', usage_metadata={'input_tokens': 7, 'output_tokens': 488, 'total_tokens': 495, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 464}})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage \n",
    "\n",
    "model.invoke([HumanMessage(content=\"Hi!, I am Dipesh\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd97fb08-ef93-4ef2-bbf0-2ab6f98338ef",
   "metadata": {},
   "source": [
    "The model does not have any concept of state. If you ask a followup questions, it can't answer. For example, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4fdf6bf6-b07c-4783-82a8-56a11ed9d42a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"I don't know your name. As an AI, I don't have access to personal information about you or any memory of past conversations.\\n\\nYou haven't told me your name. If you'd like me to refer to you by a name during this conversation, please feel free to tell me!\", additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': []}, id='run--3438491c-9aaf-444b-b0fc-de32bdccb976-0', usage_metadata={'input_tokens': 6, 'output_tokens': 514, 'total_tokens': 520, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 450}})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.invoke([HumanMessage(content=\"What is my name?\")]) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1d5629f-ee0f-40f6-b736-7a4fd735bd27",
   "metadata": {},
   "source": [
    "To fix this we need to pass the entire conversation history into the model. Let's see what happens when we do that: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3d71545c-fd31-43f8-8f4f-0b29ae0c6268",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Your name is Dipesh.', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': []}, id='run--fabf0854-8435-43ff-b45c-908de83eec2b-0', usage_metadata={'input_tokens': 28, 'output_tokens': 43, 'total_tokens': 71, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 37}})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import AIMessage\n",
    "\n",
    "model.invoke(\n",
    "    [\n",
    "        HumanMessage(content=\"Hi! I'm Dipesh.\"), \n",
    "        AIMessage(content=\"Hello Dipesh! How can I assist you today?\"), \n",
    "        HumanMessage(content=\"What's my name?\")\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c6b97fa-b448-4015-8962-e7ec71f8346d",
   "metadata": {},
   "source": [
    "## Message Persistence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2287e21e-a265-4334-94cb-bb66fc2c7d1e",
   "metadata": {},
   "source": [
    "### Persistence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1a86e20-bcd7-4dc3-9058-e6e2f518e4ea",
   "metadata": {},
   "source": [
    "LangGraph has a built-in persistence layer, implemented through checkpointers. When you compile a graph with a checkpointer, the checkpointer saves a`checkpoint` of the graph state at every super-step. Those checkpoints are saved to a `thread`, which an be accessed after graph execution.\n",
    "\n",
    "For more advanced use cases, LangGraph supports other persistence backends like SQLite or Postgres—check out the LangGraph persistence [documentation](https://langchain-ai.github.io/langgraph/concepts/persistence/?_gl=1*1f81ite*_gcl_au*MTM2NjY0NDk3My4xNzUzMTk5OTg5*_ga*MTA4ODYwNTkwNC4xNzUzMjM4NTE1*_ga_47WX3HKKY2*czE3NTM0MTM2MTkkbzQkZzEkdDE3NTM0MTM3NzEkajU3JGwwJGgw) for details.\n",
    "\n",
    "![Persistence](https://langchain-ai.github.io/langgraph/concepts/img/persistence/checkpoints.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaecc59c-3f70-4213-9d9c-21e2f1e78d0b",
   "metadata": {},
   "source": [
    "### Threads \n",
    "\n",
    "A thread is a unique ID or thread identifier assigned to each checkpoint saved by a checkpointer. It contains the accumulated state of a sequence of runs. When a run is executed, the state of the underlying graph of the assistant will be persisted to the thread.\n",
    "\n",
    "When invoking graph with a checkpointer, you must specify a thread_id as part of the configurable portion of the config:\n",
    "```python \n",
    "{\"configurable\": {\"thread_id\": \"1\"}}\n",
    "``` \n",
    "A thread's current and historical state can be retrieved. To persist state, a thread must be created prior to executing a run."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb1f0141-7a8a-43ab-bfd2-af18d82989ae",
   "metadata": {},
   "source": [
    "### Checkpoints \n",
    "\n",
    "The state of a thread at a particular point in time is called a checkpoint. Checkpoint is a snapshot of the graph state saved at each super-step and is represented by StateSnapshot object with the following key properties:\n",
    "\n",
    "- `config`: Config associated with this checkpoint.\n",
    "- `metadata`: Metadata associated with this checkpoint.\n",
    "- `values`: Values of the state channels at this point in time.\n",
    "next A tuple of the node names to execute next in the graph.\n",
    "- `tasks`: A tuple of `PregelTask` objects that contain information about next tasks to be executed. If the step was previously attempted, it will include error information. If a graph was interrupted [dynamically](https://langchain-ai.github.io/langgraph/how-tos/human_in_the_loop/add-human-in-the-loop/) from within a node, tasks will contain additional data associated with interrupts.\n",
    "  \n",
    "Checkpoints are persisted and can be used to restore the state of a thread at a later time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "d672415f-8236-4ec7-b7e6-33830204e359",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.checkpoint.memory import MemorySaver \n",
    "from langgraph.graph import START, MessagesState, StateGraph \n",
    "\n",
    "# Define a new graph \n",
    "workflow = StateGraph(state_schema=MessagesState) \n",
    "\n",
    "# Define the function that calls the model \n",
    "def call_model(state: MessagesState): \n",
    "    response = model.invoke(state[\"messages\"]) \n",
    "    return {\"messages\": response} \n",
    "\n",
    "# Define the (single) node in the graph  \n",
    "workflow.add_edge(START, \"model\") \n",
    "workflow.add_node(\"model\", call_model) \n",
    "\n",
    "# Add memory \n",
    "memory = MemorySaver() \n",
    "app = workflow.compile(checkpointer=memory) \n",
    "\n",
    "# Create a config for including threadi_id \n",
    "config = {\"configurable\": {\"thread_id\": \"abc123\"}} "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c84d83a-0c8c-4be9-914d-eff7c4ef01bc",
   "metadata": {},
   "source": [
    "We can then invoke the application "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0336b07f-ab0c-4d94-b045-b968740fcbf2",
   "metadata": {},
   "source": [
    "### Example: message inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "cec01b12-00b0-40a0-9e56-14dc96e59e76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Hi Dipesh! Nice to meet you.\n",
      "\n",
      "How can I help you today?\n"
     ]
    }
   ],
   "source": [
    "query = \"Hi! I am Dipesh.\" \n",
    "\n",
    "input_messages = [HumanMessage(query)] \n",
    "output = app.invoke({\"messages\": input_messages}, config) \n",
    "output[\"messages\"][-1].pretty_print()  # output contains all messages in state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "946862c3-68a9-4f78-9bed-0e7367f99595",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Your name is Dipesh.\n"
     ]
    }
   ],
   "source": [
    "query = \"What's my name?\" \n",
    "input_messages = [HumanMessage(query)] \n",
    "output = app.invoke({\"messages\": input_messages}, config) \n",
    "output[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f264cdd-cd4f-47d4-ba36-8786eec2a1b9",
   "metadata": {},
   "source": [
    "Now our chatbot remebers things abou us. If we change the config to reference a different `thread_id`. We can see that it starts the conversation fresh. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "2d451995-0ca7-4f77-9a00-3a238520c83f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "As an AI, I don't have access to personal information about you, including your name. I don't store personal data or remember individual users from past interactions.\n",
      "\n",
      "If you'd like me to refer to you by a specific name during our conversation, please feel free to tell me!\n"
     ]
    }
   ],
   "source": [
    "config_2 = {\"configurable\": {\"thread_id\": \"abc234\"}} \n",
    "\n",
    "input_messages = [HumanMessage(query)] \n",
    "output = app.invoke({\"messages\": input_messages}, config=config_2) \n",
    "output[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16716671-8f9d-4ba3-9b1b-dc959ecba75b",
   "metadata": {},
   "source": [
    "We can always go back to the original conversation as we are persisting it in a database. Let's pass the previous configurable `config` instead of `config_2`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "9ca32ec6-af35-45b0-92d0-a8223829bd47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Your name is Dipesh.\n"
     ]
    }
   ],
   "source": [
    "input_messages = [HumanMessage(query)]\n",
    "output = app.invoke({\"messages\": input_messages}, config)  \n",
    "output[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d63f3fc-ec77-45b9-9b42-026fb8c7aba3",
   "metadata": {},
   "source": [
    "### Async support"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14ff42f8-3ca8-416b-b3d7-a90b1130b804",
   "metadata": {},
   "source": [
    "For async support, update the `call_model` node to be an async function and use `.ainvoke` when invoking the application. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "dd1f12e3-0583-474d-a9b4-d9b6c7780851",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "As an AI, I don't have access to personal information about you, including your name. I don't store personal data or remember individual users from past interactions.\n",
      "\n",
      "If you'd like me to refer to you by a specific name during our conversation, please feel free to tell me!\n"
     ]
    }
   ],
   "source": [
    "# ansync function for node: \n",
    "async def call_model(state: MessagesState): \n",
    "    response = await model.ainvoke(state[\"messages\"]) \n",
    "    return {\"messages\": response}\n",
    "\n",
    "# Define graph as before \n",
    "workflow = StateGraph(state_schema=MessagesState) \n",
    "workflow.add_edge(START, \"model\") \n",
    "workflow.add_node(\"model\", call_model) \n",
    "app = workflow.compile(checkpointer=MemorySaver())\n",
    "\n",
    "# App invocation \n",
    "output = await app.ainvoke({\"messages\": input_messages}, config) \n",
    "output[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f312ec38-7a56-4167-b307-1649bfa2bbd6",
   "metadata": {},
   "source": [
    "As this is a new graph, it doesn't have the state of the MemoryState of the previous graph."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3d8255d-0522-46c1-ada2-57b9a46dafe8",
   "metadata": {},
   "source": [
    "### Example: dictionary inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b5b71c1-c309-4726-9f4e-68a2ec3de08d",
   "metadata": {},
   "source": [
    "LangChain runnables often accept multiple inputs via separate keys in a single dict argument. A common example is a prompt template with multiple parameters. \n",
    "\n",
    "Whereas before our runnable was a chat model, here we chain together a prompt template and chat model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "3ff26bf4-c228-47d6-8b68-b7ae6c79943b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder \n",
    "\n",
    "prompt = ChatPromptTemplate(\n",
    "    [\n",
    "        (\"system\", \"Answer in {language}\"), \n",
    "        MessagesPlaceholder(variable_name=\"messages\"), \n",
    "    ]\n",
    ") \n",
    "\n",
    "runnable = prompt | model "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e18f571-e232-42ed-869d-d09b6e10f51d",
   "metadata": {},
   "source": [
    "We then define a sigle-node graph in the same way as before. \n",
    "\n",
    "In below state:  \n",
    "- Updates to the `messages` list will append messages\n",
    "- Updates to the `language` string will overwrite the string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "8edee001-35fe-4f80-a397-f18334c998e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Sequence \n",
    "\n",
    "from langchain_core.messages import BaseMessage \n",
    "from langgraph.graph.message import add_messages \n",
    "from typing_extensions import Annotated, TypedDict \n",
    "\n",
    "class State(TypedDict): \n",
    "    messages: Annotated[Sequence[BaseMessage], add_messages] \n",
    "    language: str \n",
    "\n",
    "workflow = StateGraph(state_schema=State) \n",
    "\n",
    "def call_model(state: State): \n",
    "    response = runnable.invoke(state) \n",
    "    # update the messsage history with response \n",
    "    return {\"messages\": [response]} \n",
    "\n",
    "workflow.add_edge(START, \"model\") \n",
    "workflow.add_node(\"model\", call_model) \n",
    "\n",
    "memory = MemorySaver() \n",
    "app = workflow.compile(checkpointer=memory) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "ec7c9aa2-7986-4590-9228-ab5942cc877c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "नमस्ते, दिपेश जी।\n"
     ]
    }
   ],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"abc345\"}} \n",
    "\n",
    "input_dict = {\n",
    "    \"messages\": [HumanMessage(\"Hi, I'm Dipesh.\")], \n",
    "    \"language\": \"Nepali\",\n",
    "} \n",
    "output = app.invoke(input_dict, config) \n",
    "output[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23b28753-6178-45a1-960a-8bb2cc34f77b",
   "metadata": {},
   "source": [
    "Let's have a few more conversations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "73a959e3-9714-4250-9d51-61c720081694",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Hola, Dipesh. ¿En qué puedo ayudarte hoy?\n"
     ]
    }
   ],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"abc345\"}} \n",
    "\n",
    "input_dict = {\n",
    "    \"messages\": [HumanMessage(\"Hi, I'm Dipesh.\")], \n",
    "    \"language\": \"Spanish\",\n",
    "} \n",
    "output = app.invoke(input_dict, config) \n",
    "output[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "a2a76b0d-e761-4e6a-b9d6-2df5621fdcc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "नमस्ते दिपेश। मैं आपकी क्या सहायता कर सकता हूँ?\n"
     ]
    }
   ],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"abc345\"}} \n",
    "\n",
    "input_dict = {\n",
    "    \"messages\": [HumanMessage(\"Hi, I'm Dipesh.\")], \n",
    "    \"language\": \"Hindi\",\n",
    "} \n",
    "output = app.invoke(input_dict, config) \n",
    "output[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91cc38f1-f12e-4383-977c-c7c25362136e",
   "metadata": {},
   "source": [
    "## Managing message history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e93f34a4-a147-48f3-9089-1f43beba5802",
   "metadata": {},
   "source": [
    "### Get state \n",
    "The message history (and other elements of the application state) can be accessed via `.get_state`. \n",
    "\n",
    "When interacting with the saved graph state, you must specify a thread identifier. You can view the latest state of the graph by calling `graph.get_state(config)`. This will return a `StateSnapshot` object that corresponds to the latest checkpoint associated with the thread ID provided in the config or a checkpoint associated with a checkpoint ID for the thread, if provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "889eb3d7-b6c2-4b9d-9d69-8d694fef87b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [HumanMessage(content=\"Hi, I'm Dipesh.\", additional_kwargs={}, response_metadata={}, id='ae17fed3-2a13-4b42-8858-d1b47eb10025'),\n",
       "  AIMessage(content='नमस्ते, दिपेश जी।', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': []}, id='run--2cbcf658-6bcf-4d4e-be6e-31afb3e5cf6c-0', usage_metadata={'input_tokens': 12, 'output_tokens': 224, 'total_tokens': 236, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 218}}),\n",
       "  HumanMessage(content=\"Hi, I'm Dipesh.\", additional_kwargs={}, response_metadata={}, id='95647078-a5f3-422f-9b7a-dcfd19b111b3'),\n",
       "  AIMessage(content='Hola, Dipesh. ¿En qué puedo ayudarte hoy?', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': []}, id='run--52162666-377e-489d-b45d-89885e0264eb-0', usage_metadata={'input_tokens': 28, 'output_tokens': 103, 'total_tokens': 131, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 91}}),\n",
       "  HumanMessage(content=\"Hi, I'm Dipesh.\", additional_kwargs={}, response_metadata={}, id='ed1042db-b3a1-4aa9-a7ab-1f0201413dc0'),\n",
       "  AIMessage(content='नमस्ते दिपेश। मैं आपकी क्या सहायता कर सकता हूँ?', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': []}, id='run--fc4cc6aa-fd01-4c83-a588-5723f12bcdda-0', usage_metadata={'input_tokens': 50, 'output_tokens': 317, 'total_tokens': 367, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 305}})],\n",
       " 'language': 'Hindi'}"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state = app.get_state(config).values\n",
    "state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "57fe7a63-cc48-4c5b-a6f9-f84e6b1ab3f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Language:Hindi\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Hi, I'm Dipesh.\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "नमस्ते, दिपेश जी।\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Hi, I'm Dipesh.\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Hola, Dipesh. ¿En qué puedo ayudarte hoy?\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Hi, I'm Dipesh.\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "नमस्ते दिपेश। मैं आपकी क्या सहायता कर सकता हूँ?\n"
     ]
    }
   ],
   "source": [
    "print(f\"Language:{state['language']}\")  \n",
    "for message in state[\"messages\"]: \n",
    "    message.pretty_print() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1370923c-51c8-483b-9e57-d9eea51dea40",
   "metadata": {},
   "source": [
    "Get a `StateSnapshot` for a specific `checkpiont_id`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "ed36f8f4-aea5-488a-aa19-8861ce8c07fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StateSnapshot(values={}, next=(), config={'configurable': {'thread_id': 'abc345', 'checkpoint_id': 'run--52162666-377e-489d-b45d-89885e0264eb-0'}}, metadata=None, created_at=None, parent_config=None, tasks=(), interrupts=())"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"abc345\", \"checkpoint_id\":\"run--52162666-377e-489d-b45d-89885e0264eb-0\" }} \n",
    "state = app.get_state(config)\n",
    "state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74bf77b6-0806-47da-9564-03df62cc006e",
   "metadata": {},
   "source": [
    "### Get state history \n",
    "\n",
    "You can get the full history of the graph execution for a given thread by calling `graph.get_state_history(config)`. This will return a list of `StateSnapshot` objects associated with the thread ID provided in the config. Importantly, the checkpoints will be ordered chronologically with the most recent `checkpoint` / `StateSnapshot` being the first in the list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "0410bdb2-bb1b-4524-95ba-80cae7bdfdbd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[StateSnapshot(values={'messages': [HumanMessage(content=\"Hi, I'm Dipesh.\", additional_kwargs={}, response_metadata={}, id='ae17fed3-2a13-4b42-8858-d1b47eb10025'), AIMessage(content='नमस्ते, दिपेश जी।', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': []}, id='run--2cbcf658-6bcf-4d4e-be6e-31afb3e5cf6c-0', usage_metadata={'input_tokens': 12, 'output_tokens': 224, 'total_tokens': 236, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 218}}), HumanMessage(content=\"Hi, I'm Dipesh.\", additional_kwargs={}, response_metadata={}, id='95647078-a5f3-422f-9b7a-dcfd19b111b3'), AIMessage(content='Hola, Dipesh. ¿En qué puedo ayudarte hoy?', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': []}, id='run--52162666-377e-489d-b45d-89885e0264eb-0', usage_metadata={'input_tokens': 28, 'output_tokens': 103, 'total_tokens': 131, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 91}}), HumanMessage(content=\"Hi, I'm Dipesh.\", additional_kwargs={}, response_metadata={}, id='ed1042db-b3a1-4aa9-a7ab-1f0201413dc0'), AIMessage(content='नमस्ते दिपेश। मैं आपकी क्या सहायता कर सकता हूँ?', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': []}, id='run--fc4cc6aa-fd01-4c83-a588-5723f12bcdda-0', usage_metadata={'input_tokens': 50, 'output_tokens': 317, 'total_tokens': 367, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 305}})], 'language': 'Hindi'}, next=(), config={'configurable': {'thread_id': 'abc345', 'checkpoint_ns': '', 'checkpoint_id': '1f06984c-ebe4-63f5-8007-ee4f91990cc2'}}, metadata={'source': 'loop', 'step': 7, 'parents': {}}, created_at='2025-07-25T18:25:57.366048+00:00', parent_config={'configurable': {'thread_id': 'abc345', 'checkpoint_ns': '', 'checkpoint_id': '1f06984c-d7be-6d74-8006-f82cbab84697'}}, tasks=(), interrupts=()),\n",
       " StateSnapshot(values={'messages': [HumanMessage(content=\"Hi, I'm Dipesh.\", additional_kwargs={}, response_metadata={}, id='ae17fed3-2a13-4b42-8858-d1b47eb10025'), AIMessage(content='नमस्ते, दिपेश जी।', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': []}, id='run--2cbcf658-6bcf-4d4e-be6e-31afb3e5cf6c-0', usage_metadata={'input_tokens': 12, 'output_tokens': 224, 'total_tokens': 236, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 218}}), HumanMessage(content=\"Hi, I'm Dipesh.\", additional_kwargs={}, response_metadata={}, id='95647078-a5f3-422f-9b7a-dcfd19b111b3'), AIMessage(content='Hola, Dipesh. ¿En qué puedo ayudarte hoy?', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': []}, id='run--52162666-377e-489d-b45d-89885e0264eb-0', usage_metadata={'input_tokens': 28, 'output_tokens': 103, 'total_tokens': 131, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 91}}), HumanMessage(content=\"Hi, I'm Dipesh.\", additional_kwargs={}, response_metadata={}, id='ed1042db-b3a1-4aa9-a7ab-1f0201413dc0')], 'language': 'Hindi'}, next=('model',), config={'configurable': {'thread_id': 'abc345', 'checkpoint_ns': '', 'checkpoint_id': '1f06984c-d7be-6d74-8006-f82cbab84697'}}, metadata={'source': 'loop', 'step': 6, 'parents': {}}, created_at='2025-07-25T18:25:55.253578+00:00', parent_config={'configurable': {'thread_id': 'abc345', 'checkpoint_ns': '', 'checkpoint_id': '1f06984c-d7b9-6559-8005-719de777a5bf'}}, tasks=(PregelTask(id='611770bd-25a4-5d5a-109a-35c7b258ecba', name='model', path=('__pregel_pull', 'model'), error=None, interrupts=(), state=None, result={'messages': [AIMessage(content='नमस्ते दिपेश। मैं आपकी क्या सहायता कर सकता हूँ?', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': []}, id='run--fc4cc6aa-fd01-4c83-a588-5723f12bcdda-0', usage_metadata={'input_tokens': 50, 'output_tokens': 317, 'total_tokens': 367, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 305}})]}),), interrupts=()),\n",
       " StateSnapshot(values={'messages': [HumanMessage(content=\"Hi, I'm Dipesh.\", additional_kwargs={}, response_metadata={}, id='ae17fed3-2a13-4b42-8858-d1b47eb10025'), AIMessage(content='नमस्ते, दिपेश जी।', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': []}, id='run--2cbcf658-6bcf-4d4e-be6e-31afb3e5cf6c-0', usage_metadata={'input_tokens': 12, 'output_tokens': 224, 'total_tokens': 236, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 218}}), HumanMessage(content=\"Hi, I'm Dipesh.\", additional_kwargs={}, response_metadata={}, id='95647078-a5f3-422f-9b7a-dcfd19b111b3'), AIMessage(content='Hola, Dipesh. ¿En qué puedo ayudarte hoy?', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': []}, id='run--52162666-377e-489d-b45d-89885e0264eb-0', usage_metadata={'input_tokens': 28, 'output_tokens': 103, 'total_tokens': 131, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 91}})], 'language': 'Spanish'}, next=('__start__',), config={'configurable': {'thread_id': 'abc345', 'checkpoint_ns': '', 'checkpoint_id': '1f06984c-d7b9-6559-8005-719de777a5bf'}}, metadata={'source': 'input', 'step': 5, 'parents': {}}, created_at='2025-07-25T18:25:55.251321+00:00', parent_config={'configurable': {'thread_id': 'abc345', 'checkpoint_ns': '', 'checkpoint_id': '1f06984b-9178-609c-8004-eb08ced73095'}}, tasks=(PregelTask(id='6b0050d0-c3b4-b43b-2577-1255e2ccea2c', name='__start__', path=('__pregel_pull', '__start__'), error=None, interrupts=(), state=None, result={'messages': [HumanMessage(content=\"Hi, I'm Dipesh.\", additional_kwargs={}, response_metadata={}, id='ed1042db-b3a1-4aa9-a7ab-1f0201413dc0')], 'language': 'Hindi'}),), interrupts=()),\n",
       " StateSnapshot(values={'messages': [HumanMessage(content=\"Hi, I'm Dipesh.\", additional_kwargs={}, response_metadata={}, id='ae17fed3-2a13-4b42-8858-d1b47eb10025'), AIMessage(content='नमस्ते, दिपेश जी।', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': []}, id='run--2cbcf658-6bcf-4d4e-be6e-31afb3e5cf6c-0', usage_metadata={'input_tokens': 12, 'output_tokens': 224, 'total_tokens': 236, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 218}}), HumanMessage(content=\"Hi, I'm Dipesh.\", additional_kwargs={}, response_metadata={}, id='95647078-a5f3-422f-9b7a-dcfd19b111b3'), AIMessage(content='Hola, Dipesh. ¿En qué puedo ayudarte hoy?', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': []}, id='run--52162666-377e-489d-b45d-89885e0264eb-0', usage_metadata={'input_tokens': 28, 'output_tokens': 103, 'total_tokens': 131, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 91}})], 'language': 'Spanish'}, next=(), config={'configurable': {'thread_id': 'abc345', 'checkpoint_ns': '', 'checkpoint_id': '1f06984b-9178-609c-8004-eb08ced73095'}}, metadata={'source': 'loop', 'step': 4, 'parents': {}}, created_at='2025-07-25T18:25:21.041004+00:00', parent_config={'configurable': {'thread_id': 'abc345', 'checkpoint_ns': '', 'checkpoint_id': '1f06984b-7831-6c01-8003-b9a070e8b5f0'}}, tasks=(), interrupts=()),\n",
       " StateSnapshot(values={'messages': [HumanMessage(content=\"Hi, I'm Dipesh.\", additional_kwargs={}, response_metadata={}, id='ae17fed3-2a13-4b42-8858-d1b47eb10025'), AIMessage(content='नमस्ते, दिपेश जी।', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': []}, id='run--2cbcf658-6bcf-4d4e-be6e-31afb3e5cf6c-0', usage_metadata={'input_tokens': 12, 'output_tokens': 224, 'total_tokens': 236, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 218}}), HumanMessage(content=\"Hi, I'm Dipesh.\", additional_kwargs={}, response_metadata={}, id='95647078-a5f3-422f-9b7a-dcfd19b111b3')], 'language': 'Spanish'}, next=('model',), config={'configurable': {'thread_id': 'abc345', 'checkpoint_ns': '', 'checkpoint_id': '1f06984b-7831-6c01-8003-b9a070e8b5f0'}}, metadata={'source': 'loop', 'step': 3, 'parents': {}}, created_at='2025-07-25T18:25:18.390767+00:00', parent_config={'configurable': {'thread_id': 'abc345', 'checkpoint_ns': '', 'checkpoint_id': '1f06984b-782a-6d17-8002-f47b70e56afa'}}, tasks=(PregelTask(id='fd0157e3-e534-49a6-7b0e-c180c99f5968', name='model', path=('__pregel_pull', 'model'), error=None, interrupts=(), state=None, result={'messages': [AIMessage(content='Hola, Dipesh. ¿En qué puedo ayudarte hoy?', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': []}, id='run--52162666-377e-489d-b45d-89885e0264eb-0', usage_metadata={'input_tokens': 28, 'output_tokens': 103, 'total_tokens': 131, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 91}})]}),), interrupts=()),\n",
       " StateSnapshot(values={'messages': [HumanMessage(content=\"Hi, I'm Dipesh.\", additional_kwargs={}, response_metadata={}, id='ae17fed3-2a13-4b42-8858-d1b47eb10025'), AIMessage(content='नमस्ते, दिपेश जी।', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': []}, id='run--2cbcf658-6bcf-4d4e-be6e-31afb3e5cf6c-0', usage_metadata={'input_tokens': 12, 'output_tokens': 224, 'total_tokens': 236, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 218}})], 'language': 'Nepali'}, next=('__start__',), config={'configurable': {'thread_id': 'abc345', 'checkpoint_ns': '', 'checkpoint_id': '1f06984b-782a-6d17-8002-f47b70e56afa'}}, metadata={'source': 'input', 'step': 2, 'parents': {}}, created_at='2025-07-25T18:25:18.387854+00:00', parent_config={'configurable': {'thread_id': 'abc345', 'checkpoint_ns': '', 'checkpoint_id': '1f06976d-671c-6425-8001-59b68fb3be61'}}, tasks=(PregelTask(id='5565e7fc-1005-0d72-78aa-5d4728be8f3c', name='__start__', path=('__pregel_pull', '__start__'), error=None, interrupts=(), state=None, result={'messages': [HumanMessage(content=\"Hi, I'm Dipesh.\", additional_kwargs={}, response_metadata={}, id='95647078-a5f3-422f-9b7a-dcfd19b111b3')], 'language': 'Spanish'}),), interrupts=()),\n",
       " StateSnapshot(values={'messages': [HumanMessage(content=\"Hi, I'm Dipesh.\", additional_kwargs={}, response_metadata={}, id='ae17fed3-2a13-4b42-8858-d1b47eb10025'), AIMessage(content='नमस्ते, दिपेश जी।', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': []}, id='run--2cbcf658-6bcf-4d4e-be6e-31afb3e5cf6c-0', usage_metadata={'input_tokens': 12, 'output_tokens': 224, 'total_tokens': 236, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 218}})], 'language': 'Nepali'}, next=(), config={'configurable': {'thread_id': 'abc345', 'checkpoint_ns': '', 'checkpoint_id': '1f06976d-671c-6425-8001-59b68fb3be61'}}, metadata={'source': 'loop', 'step': 1, 'parents': {}}, created_at='2025-07-25T16:45:57.332267+00:00', parent_config={'configurable': {'thread_id': 'abc345', 'checkpoint_ns': '', 'checkpoint_id': '1f06976d-54eb-69ee-8000-dbf4e348abc7'}}, tasks=(), interrupts=()),\n",
       " StateSnapshot(values={'messages': [HumanMessage(content=\"Hi, I'm Dipesh.\", additional_kwargs={}, response_metadata={}, id='ae17fed3-2a13-4b42-8858-d1b47eb10025')], 'language': 'Nepali'}, next=('model',), config={'configurable': {'thread_id': 'abc345', 'checkpoint_ns': '', 'checkpoint_id': '1f06976d-54eb-69ee-8000-dbf4e348abc7'}}, metadata={'source': 'loop', 'step': 0, 'parents': {}}, created_at='2025-07-25T16:45:55.424907+00:00', parent_config={'configurable': {'thread_id': 'abc345', 'checkpoint_ns': '', 'checkpoint_id': '1f06976d-54e9-6404-bfff-30b1bd4103f1'}}, tasks=(PregelTask(id='539c43c0-ecd0-fc50-f402-0c6c8b39e582', name='model', path=('__pregel_pull', 'model'), error=None, interrupts=(), state=None, result={'messages': [AIMessage(content='नमस्ते, दिपेश जी।', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': []}, id='run--2cbcf658-6bcf-4d4e-be6e-31afb3e5cf6c-0', usage_metadata={'input_tokens': 12, 'output_tokens': 224, 'total_tokens': 236, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 218}})]}),), interrupts=()),\n",
       " StateSnapshot(values={'messages': []}, next=('__start__',), config={'configurable': {'thread_id': 'abc345', 'checkpoint_ns': '', 'checkpoint_id': '1f06976d-54e9-6404-bfff-30b1bd4103f1'}}, metadata={'source': 'input', 'step': -1, 'parents': {}}, created_at='2025-07-25T16:45:55.423943+00:00', parent_config=None, tasks=(PregelTask(id='45c1d208-7b2f-c346-2c49-9223f6a2747e', name='__start__', path=('__pregel_pull', '__start__'), error=None, interrupts=(), state=None, result={'messages': [HumanMessage(content=\"Hi, I'm Dipesh.\", additional_kwargs={}, response_metadata={}, id='ae17fed3-2a13-4b42-8858-d1b47eb10025')], 'language': 'Nepali'}),), interrupts=())]"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"abc345\"}}\n",
    "list(app.get_state_history(config)) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e407fa3f-87ef-460b-b187-e0ea8e03ca54",
   "metadata": {},
   "source": [
    "![image](https://langchain-ai.github.io/langgraph/concepts/img/persistence/get_state.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71ec2995-c43d-4656-bf80-8d1645ca0949",
   "metadata": {},
   "source": [
    "### Update state \n",
    "\n",
    "We can also update the state via .update_state. For example, we can manually append a new message:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "81b161c5-4ad6-48b1-832d-6c283343d5e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage \n",
    "\n",
    "_ = app.update_state(config, {\"messages\": [HumanMessage(\"Test\")]}) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "5a62ce20-e817-4fe8-8323-16850ebad767",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Language: [HumanMessage(content=\"Hi, I'm Dipesh.\", additional_kwargs={}, response_metadata={}, id='ae17fed3-2a13-4b42-8858-d1b47eb10025'), AIMessage(content='नमस्ते, दिपेश जी।', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': []}, id='run--2cbcf658-6bcf-4d4e-be6e-31afb3e5cf6c-0', usage_metadata={'input_tokens': 12, 'output_tokens': 224, 'total_tokens': 236, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 218}}), HumanMessage(content=\"Hi, I'm Dipesh.\", additional_kwargs={}, response_metadata={}, id='95647078-a5f3-422f-9b7a-dcfd19b111b3'), AIMessage(content='Hola, Dipesh. ¿En qué puedo ayudarte hoy?', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': []}, id='run--52162666-377e-489d-b45d-89885e0264eb-0', usage_metadata={'input_tokens': 28, 'output_tokens': 103, 'total_tokens': 131, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 91}}), HumanMessage(content=\"Hi, I'm Dipesh.\", additional_kwargs={}, response_metadata={}, id='ed1042db-b3a1-4aa9-a7ab-1f0201413dc0'), AIMessage(content='नमस्ते दिपेश। मैं आपकी क्या सहायता कर सकता हूँ?', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': []}, id='run--fc4cc6aa-fd01-4c83-a588-5723f12bcdda-0', usage_metadata={'input_tokens': 50, 'output_tokens': 317, 'total_tokens': 367, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 305}}), HumanMessage(content='Test', additional_kwargs={}, response_metadata={}, id='e7908077-90c9-4359-9038-030dc29f7a92')]\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Hi, I'm Dipesh.\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "नमस्ते, दिपेश जी।\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Hi, I'm Dipesh.\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Hola, Dipesh. ¿En qué puedo ayudarte hoy?\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Hi, I'm Dipesh.\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "नमस्ते दिपेश। मैं आपकी क्या सहायता कर सकता हूँ?\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Test\n"
     ]
    }
   ],
   "source": [
    "state = app.get_state(config).values \n",
    "\n",
    "print(f\"Language: {state[\"messages\"]}\") \n",
    "for message in state[\"messages\"]: \n",
    "    message.pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82ac8025-99da-4af2-aa87-18fd2e54568b",
   "metadata": {},
   "source": [
    "## Checkpointer libraries¶"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faa8921f-d103-4c6d-9b7b-c43ea5acec8d",
   "metadata": {},
   "source": [
    "Under the hood, checkpointing is powered by checkpointer objects that conform to [BaseCheckpointSaver](https://langchain-ai.github.io/langgraph/reference/checkpoints/#langgraph.checkpoint.base.BaseCheckpointSaver) interface. LangGraph provides several checkpointer implementations, all implemented via standalone, installable libraries:\n",
    "\n",
    "- `langgraph-checkpoint`: The base interface for checkpointer savers (BaseCheckpointSaver) and serialization/deserialization interface ([SerializerProtocol](https://langchain-ai.github.io/langgraph/reference/checkpoints/#langgraph.checkpoint.serde.base.SerializerProtocol)). Includes in-memory checkpointer implementation ([InMemorySaver](https://langchain-ai.github.io/langgraph/reference/checkpoints/#langgraph.checkpoint.memory.InMemorySaver)) for experimentation. LangGraph comes with `langgraph-checkpoint` included.\n",
    "- `langgraph-checkpoint-sqlite`: An implementation of LangGraph checkpointer that uses SQLite database ([SqliteSaver](https://langchain-ai.github.io/langgraph/reference/checkpoints/#langgraph.checkpoint.sqlite.SqliteSaver) / [AsyncSqliteSaver](https://langchain-ai.github.io/langgraph/reference/checkpoints/#langgraph.checkpoint.sqlite.aio.AsyncSqliteSaver)). Ideal for experimentation and local workflows. Needs to be installed separately.\n",
    "- `langgraph-checkpoint-postgres`: An advanced checkpointer that uses Postgres database ([PostgresSaver](https://langchain-ai.github.io/langgraph/reference/checkpoints/#langgraph.checkpoint.postgres.PostgresSaver) / [AsyncPostgresSaver](https://langchain-ai.github.io/langgraph/reference/checkpoints/#langgraph.checkpoint.postgres.aio.AsyncPostgresSaver)), used in LangGraph Platform. Ideal for using in production. Needs to be installed separately."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97849a87-5c1d-42b3-84ee-7779a479fef5",
   "metadata": {},
   "source": [
    "### Checkpointer interface\n",
    "Each checkpointer conforms to BaseCheckpointSaver interface and implements the following methods:\n",
    "\n",
    "- `.put` - Store a checkpoint with its configuration and metadata.\n",
    "- `.put_writes` - Store intermediate writes linked to a checkpoint (i.e. pending writes).\n",
    "- `.get_tuple` - Fetch a checkpoint tuple using for a given configuration (`thread_id` and `checkpoint_id`). This is used to populate `StateSnapshot` in `graph.get_state()`.\n",
    "- `.list` - List checkpoints that match a given configuration and filter criteria. This is used to populate state history in `graph.get_state_history()`.\n",
    "\n",
    "If the checkpointer is used with asynchronous graph execution (i.e. executing the graph via `.ainvoke`, `.astream`, `.abatch`), asynchronous versions of the above methods will be used (`.aput`, `.aput_writes`, `.aget_tuple`, `.alist`)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6993dc67-f7a5-46ea-ab40-35197274673d",
   "metadata": {},
   "source": [
    "## References \n",
    "\n",
    "- LangChain - [Build a Chatbot](https://python.langchain.com/docs/tutorials/chatbot/)\n",
    "- LangGraph Documentation - [Persistence](https://python.langchain.com/docs/tutorials/chatbot/)\n",
    "- LangChain - [How to add messages history?](https://python.langchain.com/docs/how_to/message_history/)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c7d3439-b7d6-456b-8fa2-c71439d2b9ec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
