{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc4ce3a0-a5bb-4616-8f83-9fc1b4c15a17",
   "metadata": {},
   "source": [
    "# 01. LangChain: A Beginner's Guide"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd17e167-2a7c-4b56-ae60-7c5924d36821",
   "metadata": {},
   "source": [
    "This notebook covers the following content: \n",
    "1. Loading Model\n",
    "2. Prompts - `ChatMessage` & `ChatPromptTemplate`\n",
    "3. Runnables & LangChain Expression Language\n",
    "4. Chaining \n",
    "5. Streaming\n",
    "6. How to Guide Generation With Context?\n",
    "7. Debugging - `set_debug`, `set_verbose`, `LangSmith Tracing`, `astream_events`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74386f2a-e33c-4932-8819-a6bbda61aa09",
   "metadata": {},
   "source": [
    "## 1. Loading Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60ed0a5b-6062-4378-8e7d-54494bc7c9a1",
   "metadata": {},
   "source": [
    "### Setup environment variable for LLM API Key"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b90d1086-536c-440f-862e-643aaaae3db8",
   "metadata": {},
   "source": [
    "In the root directory of your project, create a file named `.env` to securely store your API key. Add the following line to the `.env` file:\n",
    "```text\n",
    "GEMINI_API_KEY = \"XXXXXXXXXXXXXXXXX\"\n",
    "```\n",
    "Make sure to replace \"XXXXXXXXXXXXXXXXX\" with your actual Gemini API key or your preferred LLM API key (for example, OpenAI or Anthropic)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7ed77b5-1a9b-4b94-9a6b-9308b0be1d70",
   "metadata": {},
   "source": [
    "### Get `.env` variable for LLM API Key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5037c3bc-046e-40d3-bfbb-7f0e355d9d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load .env file from the project root\n",
    "load_dotenv()\n",
    "\n",
    "gemini_api_key = os.getenv(\"GEMINI_API_KEY\")\n",
    "\n",
    "if gemini_api_key is None:\n",
    "    raise ValueError(\"GEMINI_API_KEY not found. Please set it in the .env file.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51877b17-f308-4b90-a910-668b9dfe9121",
   "metadata": {},
   "source": [
    "### Initialize the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "31dd13a5-1970-457a-97a9-da97b4cfe29a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "chat_model = ChatGoogleGenerativeAI(\n",
    "    model = \"gemini-2.5-flash\", \n",
    "    temperature = 0, \n",
    "    api_key = gemini_api_key\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "16c2f3ea-0035-422d-805c-8a4fe4835d2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Okay, here\\'s one I just brewed up:\\n\\nWhy did the AI go to the coffee shop?\\n... It heard humans needed a \"java update\" to start their day!', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': []}, id='run--d1ab79ff-c12b-4d51-8ce4-8e82bbe7687b-0', usage_metadata={'input_tokens': 7, 'output_tokens': 1406, 'total_tokens': 1413, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 1368}})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = chat_model.invoke(\"Tell me a new joke!\")\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1762ab11-4445-49ce-88f3-20ccb4d19c2d",
   "metadata": {},
   "source": [
    "The output might seem disorganized with additional metadata, but you can easily extract the text response by using the `.content` attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bebd7d50-df75-4819-a53c-0ec159de42ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Okay, here\\'s one I just brewed up:\\n\\nWhy did the AI go to the coffee shop?\\n... It heard humans needed a \"java update\" to start their day!'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4e1406a-24d0-49d9-898d-4ae0d7fddc2c",
   "metadata": {},
   "source": [
    "## 2. Prompts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46a8a556-9053-4b51-845a-382e8e251972",
   "metadata": {},
   "source": [
    "### Using `ChatMessages` from `langchain_core.messages` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8329540c-c270-4948-aa34-37754caef673",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Ciao!'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(\"Translate the following from English into Italian\"),\n",
    "    HumanMessage(\"hi!\"),\n",
    "]\n",
    "\n",
    "chat_model.invoke(messages).content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04936666-d8fd-450a-bb69-49d34334db72",
   "metadata": {},
   "source": [
    "### Using String `PromptTemplate`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "220d7a32-be84-4491-b127-1dc59eb0a0cd",
   "metadata": {},
   "source": [
    "LangChain also supports chat model inputs via strings or `OpenAI` format. The following are equivalent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "75d12985-e3ca-4ca4-9929-caabcad49cc3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello there! How can I help you today?'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# chat_model.invoke(\"Hello\")\n",
    "# chat_model.invoke([{\"role\": \"user\", \"content\": \"Hello\"}])\n",
    "chat_model.invoke([HumanMessage(\"Hello\")]).content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28b01cfb-4da7-4998-9053-99dd6a7ac1e8",
   "metadata": {},
   "source": [
    "### Using `ChatPromptTemplate` from `langchain_core.prompts`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eff342bd-3066-4615-a959-d13ad461fc12",
   "metadata": {},
   "source": [
    "![image.png](https://www.freecodecamp.org/news/content/images/2024/04/prompt_and_model--1-.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7f6c110b-358d-4eb5-a1c2-fe615374f963",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate \n",
    "\n",
    "joke_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a world class comedian.\"), \n",
    "    (\"human\", \"Tell me a joke about {topic}\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b7cc03af-70e4-479c-ad6d-fc9318468db1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptValue(messages=[SystemMessage(content='You are a world class comedian.', additional_kwargs={}, response_metadata={}), HumanMessage(content='Tell me a joke about beets', additional_kwargs={}, response_metadata={})])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joke_prompt.invoke({\"topic\": \"beets\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "429010e5-a922-47fe-a845-91eb65ca701c",
   "metadata": {},
   "source": [
    "- You construct a prompt template consisting of templates for a `SystemMessage` and a `HumanMessage` using `from_messages`.\n",
    "- You can think of `SystemMessages` as meta-instructions that are not part of the current conversation, but purely guide input.\n",
    "- The prompt template contains `{topic}` in curly braces. This denotes a required parameter named `\"topic\"`.\n",
    "- You invoke the prompt template with a dict with a key named `\"topic\"` and a value`\"beets\"`.\n",
    "- The result contains the formatted messages."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f6de9cb-e01e-4e2f-8236-eaea13128672",
   "metadata": {},
   "source": [
    "## 3. Runnables & LangChain Expression Language\n",
    "\n",
    "**Runnable** in LangChain is a key class that acts as a building block for structuring AI workflows. A Runnable represents a unit of work—like a function or a model call—that can be:\n",
    "- **Invoked** (run on a single input),\n",
    "- **Batched** (run on multiple inputs in parallel),\n",
    "- **Streamed** (produce output as it’s being generated),\n",
    "- **Transformed** (change its behavior or output),\n",
    "- **Composed** (combined with other Runnables into workflows).\n",
    "\n",
    "Runnables provide a standard interface for connecting different steps in your workflow. This means you can easily set up a process where the output from one step is automatically passed as input to the next, creating efficient linear or complex chains of operations.\n",
    "\n",
    "Runnables are supported by several core methods:\n",
    "- `invoke / ainvoke`: Run synchronously or asynchronously\n",
    "- `batch / abatch`: Process many inputs in parallel\n",
    "- `stream / astream`: Stream output as soon as it’s available\n",
    "\n",
    "You can build chains using LCEL (LangChain Expression Language) with these components, combining simple tasks into more advanced workflows by methods like RunnableSequence (run steps one after another) or RunnableParallel (run steps at the same time).\n",
    "\n",
    "What can you do with Runnables?\n",
    "- `pipe` (or the `|` operator): Connects multiple Runnables in sequence so output flows from one to the next.\n",
    "- `fromMap`: Runs multiple Runnables at the same time on the same input, useful for parallel processing.\n",
    "- `passthrough`: Passes the input directly to the output without changes — acts like a placeholder or a no-op.\n",
    "- `mapInput` and `mapInputStream`: Modify or transform input values or input streams before running the Runnable.\n",
    "- `fromFunction`: Turns any regular function into a Runnable, allowing easy integration.\n",
    "`fromRouter`: Routes inputs dynamically to different Runnables based on conditions or logic."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c2fbdd9-6afe-431a-9241-440bd3238b59",
   "metadata": {},
   "source": [
    "**LangChain Expression Language (LCEL)** is a simple, powerful way to build custom chains of operations using the Runnable protocol. Its core idea is that you can chain any two Runnables together sequentially—the output of one Runnable’s `invoke()` method becomes the input to the next. This chaining can be done using the pipe operator `|` or the equivalent `.pipe()` method, both of which create a `RunnableSequence.`\n",
    "\n",
    "The resulting `RunnableSequence` is itself a runnable, which means it can be **invoked**, **streamed**, or further **chained** just like any other runnable.\n",
    "\n",
    "**When to use LCEL?** \n",
    "- If you are making a single LLM call, you don't need LCEL; instead call the underlying chat model directly.\n",
    "- If you have a simple chain (e.g., prompt + llm + parser, simple retrieval set up etc.), LCEL is a reasonable fit, if you're taking advantage of the LCEL benefits.\n",
    "- If you're building a complex chain (e.g., with branching, cycles, multiple agents, etc.) use LangGraph instead. Remember that you can always use LCEL within individual nodes in LangGraph."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "749c5ef3-67b4-4ab4-beae-7bcd797c5bcb",
   "metadata": {},
   "source": [
    "## 4. Chaining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "976ae58c-d9a7-4b2c-b8c2-8ef9d3255015",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "chain = joke_prompt | chat_model | StrOutputParser() "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "845d6ed9-84cd-40c7-b591-32b87d19592c",
   "metadata": {},
   "source": [
    "![image.png](https://www.freecodecamp.org/news/content/images/2024/04/prompt_model_and_output_parser--1-.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f77d9d43-97e8-44c4-9cc4-4b4f0bd2321c",
   "metadata": {},
   "source": [
    "You still pass `{\"topic\": \"beets\"}` as input to the new `chain` because the first `Runnable` in the sequence is still the `PromptTemplate` you declared before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a37d6993-0b8e-4b2d-bfd9-d380193badce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Alright, alright, settle down folks, you\\'re too kind!\\n\\nSo, I was at a fancy restaurant the other night, and the waiter comes over, all serious, and he\\'s describing the beet salad. He says, \"It\\'s an earthy delight, with a hint of sweetness, and a vibrant color that just *pops*!\"\\n\\nAnd I\\'m sitting there, looking at this plate of purple-red stuff, and I just thought to myself...\\n\\n\"Honestly, what *is* it about beets that makes some people absolutely *love* them, and others just can\\'t *stand* them?\"\\n\\nAnd then it hit me.\\n\\n**Beats me!**\\n\\n*(Pause for groans and applause)*\\n\\nThank you, I\\'ll be here all week! Try the beet salad! Or don\\'t! It\\'s your life!'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"topic\": \"beets\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f50294d-b304-49d0-a45c-7cc8ac2ed144",
   "metadata": {},
   "source": [
    "## 5. Streaming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ecd36b1-5b0e-4d67-b91c-ef0e622aae3e",
   "metadata": {},
   "source": [
    "All Runnable objects have two important methods:\n",
    "\n",
    "- `stream`: a synchronous (sync) method.\n",
    "- `astream`: an asynchronous (async) variant.\n",
    "\n",
    "These methods return a generator. A generator lets you get output as soon as it is ready, instead of waiting for everything to finish. This helps your application update quickly when new data becomes available.\n",
    "\n",
    "**Why Use Streaming?**\n",
    "- Large language models can take several seconds to reply fully to a query.\n",
    "- This response time is much slower than the typical 200-300ms it takes for an app to feel instantly responsive to users.\n",
    "\n",
    "**How to Improve Responsiveness?**\n",
    "- The main way to keep your app feeling fast is to show progress while you wait for the final result.\n",
    "- By streaming the model's output token by token (a token is a word or part of a word), you let users see answers as they're being generated.\n",
    "\n",
    ">**In summary:**\n",
    "Using `stream` or `astream` allows you to present results immediately and improve the user experience by streaming output progressively as it is created."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97976df4-31b1-463f-8fa2-dd64643dd0f3",
   "metadata": {},
   "source": [
    "### sync `stream` API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e8333b95-0092-4232-b9bf-7535d9da3651",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The most common and well-known color of the sky during the day is **|blue**.\n",
      "\n",
      "However, the sky's color can change dramatically depending on several factors:\n",
      "\n",
      "*   **Sunrise and Sunset:** It often appears in shades of **red, orange, pink, and purple** due to the way sunlight is scattered when| it travels through more of the Earth's atmosphere at these times.\n",
      "*   **Overcast or Stormy Weather:** It can look **gray or dark**.\n",
      "*   **Night:** The sky appears **black or very dark blue**,| with stars visible.\n",
      "*   **Atmospheric Conditions:** Dust, pollution, or smoke can sometimes give the sky a hazy, yellowish, or brownish tint.\n",
      "\n",
      "So, while **blue** is the default answer, the sky is actually| a dynamic canvas of many colors!|"
     ]
    }
   ],
   "source": [
    "chunks = []\n",
    "for chunk in chat_model.stream(\"what color is the sky?\"): \n",
    "    chunks.append(chunk)\n",
    "    print(chunk.content, end='|', flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a1782b4-87e3-4d78-9c35-7297b7171564",
   "metadata": {},
   "source": [
    ">You can also stream output through chains because chains themselves are Runnables. Let's try sync `stream` in chain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aef42f1f-092e-449e-b165-bb730748aa8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alright, alright, settle down folks, you're too kind! I love this crowd|, you're like a pack of hyenas... but in a good way!\n",
      "\n",
      "So, I was thinking about tigers the other day, majestic creatures, right? Powerful, graceful... terrible golfers.\n",
      "\n",
      "No, seriously!\n",
      "\n",
      "**|Why did the tiger get kicked off the golf course?**\n",
      "\n",
      "...Because every time he lined up a shot, he'd just pounce on the ball and try to eat it!\n",
      "\n",
      "*Thank you, I'll be here all| week, try the gazelle!*|"
     ]
    }
   ],
   "source": [
    "for chunk in chain.stream({\"topic\": \"tiger\"}): \n",
    "    print(chunk, end='|', flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8083b42-f996-4439-a7c4-836e5bf17983",
   "metadata": {},
   "source": [
    "### async `astream` API\n",
    "\n",
    "If you're working in an async environment, you may consider using the async `astream` API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "216db84f-f7ae-433c-b760-9dd7acbb442e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The color of the sky is most commonly **blue** during the day.\n",
      "\n",
      "However, its color can vary significantly depending on the time of day, weather conditions, and atmospheric factors:\n",
      "\n",
      "|1.  **Blue (Daytime):** This is due to a phenomenon called **Rayleigh scattering**. Sunlight, which appears white, is actually made up of all colors of the rainbow. When sunlight enters Earth's atmosphere, the| shorter wavelengths (like blue and violet) are scattered more efficiently by the tiny nitrogen and oxygen molecules than the longer wavelengths (like red and yellow). Because blue light is scattered in all directions, it makes the sky appear blue to our eyes.\n",
      "\n",
      "2|.  **Red, Orange, Pink, Yellow (Sunrise/Sunset):** At sunrise and sunset, the sun's light has to travel through much more of the atmosphere to reach our eyes. By the time it gets to us, most of| the blue and violet light has been scattered away, leaving the longer wavelengths (red, orange, yellow) to dominate the sky's color. Dust, pollution, and water vapor in the atmosphere can enhance these colors.\n",
      "\n",
      "3.  **|Grey or White (Cloudy/Overcast):** Clouds are made of water droplets or ice crystals, which are much larger than air molecules. These larger particles scatter all wavelengths of light equally (a process called Mie scattering), making the clouds appear white or| grey. Thicker clouds block more light, making them appear darker grey.\n",
      "\n",
      "4.  **Dark Grey or Black (Stormy/Night):** During severe storms, very thick clouds block almost all sunlight, making the sky appear very| dark. At night, without the sun's light, the sky appears black, though light pollution or moonlight can give it a very dark blue or grey tint.\n",
      "\n",
      "5.  **Other Variations:**\n",
      "    *   Sometimes, during| severe thunderstorms, the sky can take on a **greenish** hue, possibly due to the way light interacts with large amounts of water in the storm clouds.\n",
      "    *   Smoke from wildfires or significant air pollution can make the sky appear **|hazy, brownish, or yellowish**.\n",
      "\n",
      "So, while we typically say the sky is blue, it's a dynamic canvas of colors!|"
     ]
    }
   ],
   "source": [
    "chunks = [] \n",
    "async for chunk in chat_model.astream(\"what is the color of the sky?\"): \n",
    "    chunks.append(chunk) \n",
    "    print(chunk.content, end='|', flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3f583e4d-c5ff-41e5-b981-e4832baf57fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessageChunk(content='The color of the sky is most commonly **blue** during the day.\\n\\nHowever, its color can vary significantly depending on the time of day, weather conditions, and atmospheric factors:\\n\\n', additional_kwargs={}, response_metadata={'safety_ratings': []}, id='run--378aab53-ec21-4c4b-9b6f-60139bcb08d5', usage_metadata={'input_tokens': 9, 'output_tokens': 852, 'total_tokens': 861, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 815}})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "622b992e-5a8c-4b14-a258-00dede8a12f1",
   "metadata": {},
   "source": [
    "## 6. How to Guide Generation with Context?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ced68945-79ee-4826-8f25-281f11be42e7",
   "metadata": {},
   "source": [
    "Large language models (LLMs) are trained on huge amounts of data and contain broad “knowledge” about many topics. However, to get more accurate or specific answers, it’s common to provide the model with extra context or private data relevant to the question. This approach is the core idea behind **Retrieval-Augmented Generation (RAG).**\n",
    "\n",
    "```\n",
    "Prompt = Query + Relevant Context  -->  LLM  -->  More Accurate and Useful Response\n",
    "```\n",
    "\n",
    "For example, LLMs don’t know the current date because they are trained on data up until a certain cutoff. So, if you want the model to answer based on today’s date, you explicitly supply it as context:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b0b184ce-2594-40c0-b7bd-4cb66cf9e35f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I do not have access to real-time information, including the current date and time. My knowledge cutoff is typically a specific point in the past, and I don't have a built-in clock or the ability to browse the live internet.\\n\\nTo find the current date and time, please check your device (computer, phone, tablet) or do a quick search online.\""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_model.invoke(\"What is the current date and time?\").content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f900024-1d85-4e70-bf1b-7359baf8e35d",
   "metadata": {},
   "source": [
    "Now, let's see what happens when you give the model the current data as context:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "356e7e15-111d-4a89-b120-27aa6c47667d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The current date is 2025-07-22.'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datetime import date \n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", 'You know that the current data is {current_date}.'), \n",
    "    (\"human\", \"{question}\")\n",
    "])\n",
    "\n",
    "chain = prompt | chat_model | StrOutputParser() \n",
    "\n",
    "chain.invoke({\n",
    "    \"question\": \"What is the current date?\", \n",
    "    \"current_date\": date.today()\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4a7226e-f4cc-491b-aba4-e8cb0138f68b",
   "metadata": {},
   "source": [
    "Let's try to do something cool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cc88d3d2-35ff-4b59-959a-3f4a9b4c3d7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Okay, you have roughly **7-9 days** until the end of July. This is a very tight timeframe, so we need to be highly focused and efficient. This plan assumes you have *some* existing ML knowledge and are looking to refresh and solidify it, not learn from scratch.\n",
       "\n",
       "**Goal:** Cover the most frequently asked ML interview topics, practice coding, and prepare for behavioral questions.\n",
       "\n",
       "---\n",
       "\n",
       "## ML Interview Prep Plan (7-9 Days)\n",
       "\n",
       "**Core Principle:** Active recall and practice over passive reading. Explain concepts aloud, write code, and simulate interview scenarios.\n",
       "\n",
       "---\n",
       "\n",
       "### **Phase 1: ML Fundamentals & Core Algorithms (Days 1-2)**\n",
       "\n",
       "**Focus:** Solidify theoretical understanding of key concepts and algorithms.\n",
       "\n",
       "*   **Day 1: ML Basics & Regression/Classification**\n",
       "    *   **Concepts:**\n",
       "        *   Supervised vs. Unsupervised Learning\n",
       "        *   Bias-Variance Tradeoff (Crucial!)\n",
       "        *   Overfitting & Underfitting, Regularization (L1, L2)\n",
       "        *   Loss Functions (MSE, Cross-Entropy)\n",
       "        *   Gradient Descent (Intuition, types: Batch, SGD, Mini-Batch)\n",
       "    *   **Algorithms:**\n",
       "        *   **Linear Regression:** Assumptions, how it works, interpretation.\n",
       "        *   **Logistic Regression:** How it works (sigmoid), probability interpretation, decision boundary.\n",
       "    *   **Action:**\n",
       "        *   Review your notes or quick online summaries (e.g., towardsdatascience, GeeksforGeeks).\n",
       "        *   Explain each concept/algorithm aloud as if teaching someone.\n",
       "        *   Draw simple diagrams for bias-variance, decision boundaries.\n",
       "\n",
       "*   **Day 2: Tree-Based Models & Ensemble Methods**\n",
       "    *   **Algorithms:**\n",
       "        *   **Decision Trees:** How they work (splitting criteria: Gini, Entropy), pros/cons.\n",
       "        *   **Random Forests:** Bagging, how it improves DTs, feature importance.\n",
       "        *   **Gradient Boosting (XGBoost/LightGBM):** Boosting concept, sequential learning, key differences from RF. (Focus on intuition, not deep math).\n",
       "        *   **SVM (Support Vector Machines):** Intuition of hyperplane, kernel trick (briefly).\n",
       "    *   **Action:**\n",
       "        *   Compare and contrast these algorithms: When to use which? What are their strengths/weaknesses?\n",
       "        *   Understand the difference between bagging and boosting.\n",
       "        *   Quickly review K-Means (unsupervised clustering) and PCA (dimensionality reduction) intuition.\n",
       "\n",
       "---\n",
       "\n",
       "### **Phase 2: Evaluation, Feature Engineering & Practical ML (Days 3-4)**\n",
       "\n",
       "**Focus:** How to evaluate models, prepare data, and handle practical aspects.\n",
       "\n",
       "*   **Day 3: Model Evaluation & Hyperparameter Tuning**\n",
       "    *   **Metrics (Classification):** Accuracy, Precision, Recall, F1-Score, ROC-AUC, Confusion Matrix. (Know when to use which!)\n",
       "    *   **Metrics (Regression):** MSE, RMSE, MAE, R-squared.\n",
       "    *   **Techniques:** Cross-Validation (K-Fold), Train-Validation-Test Split.\n",
       "    *   **Hyperparameter Tuning:** Grid Search, Random Search (briefly).\n",
       "    *   **Action:**\n",
       "        *   Work through examples of calculating metrics given a confusion matrix.\n",
       "        *   Understand the implications of different types of errors (false positives vs. false negatives).\n",
       "        *   Explain why cross-validation is important.\n",
       "\n",
       "*   **Day 4: Feature Engineering & Data Preprocessing**\n",
       "    *   **Concepts:**\n",
       "        *   Handling Missing Values (Imputation strategies)\n",
       "        *   Categorical Encoding (One-Hot, Label Encoding)\n",
       "        *   Feature Scaling (Standardization, Normalization)\n",
       "        *   Outlier Detection/Handling (briefly)\n",
       "        *   Feature Selection (basic methods: correlation, importance)\n",
       "    *   **Action:**\n",
       "        *   Think about a dataset you've worked with: How would you preprocess it step-by-step?\n",
       "        *   Practice explaining common feature engineering techniques.\n",
       "        *   Review `pandas` and `numpy` basics for data manipulation.\n",
       "\n",
       "---\n",
       "\n",
       "### **Phase 3: Coding & System Design (Days 5-6)**\n",
       "\n",
       "**Focus:** Demonstrate your coding proficiency and ability to think about ML in production.\n",
       "\n",
       "*   **Day 5: Python & ML Libraries Coding**\n",
       "    *   **Python Basics:** Data structures (lists, dicts, sets, tuples), string manipulation, functions, classes (basic OOP).\n",
       "    *   **`pandas`:** Data loading, filtering, grouping, merging, pivot tables.\n",
       "    *   **`numpy`:** Array operations, broadcasting.\n",
       "    *   **`scikit-learn`:** Model training pipeline (import, instantiate, fit, predict, evaluate).\n",
       "    *   **Action:**\n",
       "        *   Solve 2-3 LeetCode Easy/Medium problems (focus on array/string manipulation, hash maps).\n",
       "        *   Implement a simple ML pipeline using `sklearn` (e.g., train a Logistic Regression on a dummy dataset).\n",
       "        *   Practice common `pandas` operations (e.g., find top N items, calculate rolling average).\n",
       "\n",
       "*   **Day 6: ML System Design & Project Deep Dive**\n",
       "    *   **ML System Design:**\n",
       "        *   **Concepts:** MLOps lifecycle (data collection, training, deployment, monitoring, retraining).\n",
       "        *   **Questions:** \"How would you design a spam detector?\", \"How would you build a recommendation system?\", \"How would you monitor a deployed model for data drift/concept drift?\"\n",
       "        *   **Key Elements:** Data pipelines, model serving, A/B testing, logging, alerting.\n",
       "    *   **Your Projects:**\n",
       "        *   **Prepare 1-2 projects** you've worked on in detail.\n",
       "        *   **STAR Method:** Situation, Task, Action, Result.\n",
       "        *   **Be ready to discuss:** Why you chose certain models/features, challenges faced, how you evaluated, what you'd do differently.\n",
       "    *   **Action:**\n",
       "        *   Brainstorm components for a simple ML system (e.g., a movie recommender).\n",
       "        *   Practice explaining your projects using the STAR method. Anticipate follow-up questions.\n",
       "\n",
       "---\n",
       "\n",
       "### **Phase 4: Mock Interview & Final Review (Day 7-9)**\n",
       "\n",
       "**Focus:** Consolidate knowledge, practice under pressure, and address weaknesses.\n",
       "\n",
       "*   **Day 7: Mock Interview & Weakness Targeting**\n",
       "    *   **Mock Interview:** If possible, do a mock interview with a friend or use a platform like Pramp. This is invaluable for identifying gaps and practicing communication.\n",
       "    *   **Self-Assessment:** After the mock, or after reviewing the past days, identify your weakest areas.\n",
       "    *   **Targeted Review:** Spend 2-3 hours diving deeper into those specific weak spots.\n",
       "    *   **Action:**\n",
       "        *   Conduct a mock interview.\n",
       "        *   Prioritize 1-2 topics for deeper review based on your performance.\n",
       "\n",
       "*   **Day 8-9 (Buffer/Final Polish):**\n",
       "    *   **Review High-Yield Topics:** Go over your notes for Bias-Variance, Overfitting/Regularization, Key Metrics, and your chosen projects.\n",
       "    *   **Behavioral Questions:**\n",
       "        *   \"Tell me about yourself.\" (Craft a concise, compelling story)\n",
       "        *   \"Why ML?\" / \"Why this company?\"\n",
       "        *   \"Strengths/Weaknesses.\"\n",
       "        *   \"How do you handle conflict/failure?\"\n",
       "    *   **Questions for Interviewer:** Prepare 2-3 thoughtful questions to ask at the end.\n",
       "    *   **Rest:** Get good sleep! Don't cram the night before.\n",
       "\n",
       "---\n",
       "\n",
       "### **Daily Habits (Throughout the 7-9 Days):**\n",
       "\n",
       "*   **Explain Aloud:** For every concept, try to explain it simply and clearly.\n",
       "*   **Whiteboard Practice:** Draw diagrams for algorithms, system designs, or data flows.\n",
       "*   **Quick Code Snippets:** Implement small parts of algorithms or data processing steps.\n",
       "*   **Review Your Resume/Projects:** Be able to speak confidently about everything listed.\n",
       "*   **Stay Hydrated & Take Breaks:** Avoid burnout.\n",
       "\n",
       "---\n",
       "\n",
       "**What to Skip (for now, given the time constraint):**\n",
       "\n",
       "*   Deep dives into advanced Deep Learning architectures (Transformers, GANs, etc.) unless the role explicitly requires it.\n",
       "*   Complex mathematical proofs for algorithms. Focus on intuition.\n",
       "*   Niche ML algorithms unless they are directly relevant to the job description.\n",
       "\n",
       "Good luck! You can make significant progress with this focused approach."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, Markdown\n",
    "\n",
    "response = chain.invoke({\n",
    "    \"question\": \"I need to prepare for ML interview by the end of this month, give me a short structured plan\", \n",
    "    \"current_date\": date.today()\n",
    "})\n",
    "\n",
    "display(Markdown(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c558975-0608-4a62-adef-572e6bcbdd2d",
   "metadata": {},
   "source": [
    ">**Note:**\n",
    "Augmenting generation with additional context is a complex and rich topic. In practical applications, this often involves providing the language model with a large document or relevant sections retrieved from an external data source, such as a financial report. Retrieval-Augmented Generation (RAG) is a powerful technique that enables models to answer questions based on vast amounts of information by first retrieving relevant data and then using it to generate accurate responses."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b96bc892-d325-4439-a2f3-5679b1aa3831",
   "metadata": {},
   "source": [
    "## 7. Debugging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4317c34-7a15-4aec-bdb1-bf015efee79c",
   "metadata": {},
   "source": [
    "Language models (LLMs) are inherently non-deterministic, meaning they can produce different outputs on the same input. As your chains grow more complex, understanding the internal data flow becomes critically important for diagnosing issues and improving performance.\n",
    "\n",
    "There are three main methods for debugging:\n",
    "- **Verbose Mode**: This adds print statements for \"important\" events in your chain.\n",
    "- **Debug Mode**: This add logging statements for ALL events in your chain.\n",
    "- **LangSmith Tracing**: This logs events to LangSmith to allow for visualization there.\n",
    "\n",
    "|                       | Verbose Mode | Debug Mode | LangSmith Tracing |\n",
    "|-----------------------|--------------|------------|-------------------|\n",
    "| **Free**              | ✅           | ✅         | ✅                |\n",
    "| **UI**                | ❌           | ❌         | ✅                |\n",
    "| **Persisted**         | ❌           | ❌         | ✅                |\n",
    "| **See all events**    | ❌           | ✅         | ✅                |\n",
    "| **See \"important\" events** | ✅      | ❌         | ✅                |\n",
    "| **Runs Locally**      | ✅           | ✅         | ❌                |\n",
    "\n",
    ">You can check out this [guide](https://python.langchain.com/docs/how_to/debugging/) by LangChain on how to debug your LLM apps\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82a9014b-8dc1-4d55-905a-3e95a74b44a1",
   "metadata": {},
   "source": [
    "### `set_debug`\n",
    "LangChain provides a global debugging utility called `set_debug()` that enables detailed logging of all chain internals. When enabled, it logs every input and output passing through components like chains, models, agents, tools, and retrievers, helping you trace exactly what data is processed step-by-step."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fe044b0-18a7-43ac-bc85-ea86c5f80755",
   "metadata": {},
   "source": [
    "Install `LangChain` if you haven't already.\n",
    "```\n",
    "pip install langchain\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1196cf31-cfd8-40ea-954e-497db7fbf545",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > prompt:ChatPromptTemplate] Entering Prompt run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > prompt:ChatPromptTemplate] [1ms] Exiting Prompt run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:RunnableSequence > llm:ChatGoogleGenerativeAI] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"System: You know that the current date is \\\"2025-07-22\\\".\\nHuman: What is the current date?\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:RunnableSequence > llm:ChatGoogleGenerativeAI] [915ms] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"The current date is July 22, 2025.\",\n",
      "        \"generation_info\": {\n",
      "          \"finish_reason\": \"STOP\",\n",
      "          \"model_name\": \"gemini-2.5-flash\",\n",
      "          \"safety_ratings\": []\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"The current date is July 22, 2025.\",\n",
      "            \"response_metadata\": {\n",
      "              \"prompt_feedback\": {\n",
      "                \"block_reason\": 0,\n",
      "                \"safety_ratings\": []\n",
      "              },\n",
      "              \"finish_reason\": \"STOP\",\n",
      "              \"model_name\": \"gemini-2.5-flash\",\n",
      "              \"safety_ratings\": []\n",
      "            },\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run--cda34da6-db06-4bf7-a6f6-27d878ef1cb6-0\",\n",
      "            \"usage_metadata\": {\n",
      "              \"input_tokens\": 26,\n",
      "              \"output_tokens\": 53,\n",
      "              \"total_tokens\": 79,\n",
      "              \"input_token_details\": {\n",
      "                \"cache_read\": 0\n",
      "              },\n",
      "              \"output_token_details\": {\n",
      "                \"reasoning\": 38\n",
      "              }\n",
      "            },\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": {\n",
      "    \"prompt_feedback\": {\n",
      "      \"block_reason\": 0,\n",
      "      \"safety_ratings\": []\n",
      "    }\n",
      "  },\n",
      "  \"run\": null,\n",
      "  \"type\": \"LLMResult\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > parser:StrOutputParser] Entering Parser run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > parser:StrOutputParser] [0ms] Exiting Parser run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"The current date is July 22, 2025.\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence] [918ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"The current date is July 22, 2025.\"\n",
      "}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The current date is July 22, 2025.'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.globals import set_debug\n",
    "from datetime import date\n",
    "\n",
    "set_debug(True)\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", 'You know that the current date is \"{current_date}\".'),\n",
    "    (\"human\", \"{question}\")\n",
    "])\n",
    "\n",
    "chain = prompt | chat_model | StrOutputParser()\n",
    "\n",
    "chain.invoke({\n",
    "    \"question\": \"What is the current date?\",\n",
    "    \"current_date\": date.today()\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c5b4c98-694c-4619-93a4-a0fc2f7bdadf",
   "metadata": {},
   "source": [
    "### `set_verbose`\n",
    "Setting the verbose flag will print out inputs and outputs in a slightly more readable format and will skip logging certain raw outputs (like the token usage stats for an LLM call) so that you can focus on application logic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7e231d20-8e6a-40e3-9c39-c54297239530",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The current date is July 22, 2025.'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.globals import set_verbose\n",
    "from datetime import date\n",
    "\n",
    "set_debug(False)\n",
    "set_verbose(True) \n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", 'You know that the current date is \"{current_date}\".'),\n",
    "    (\"human\", \"{question}\")\n",
    "])\n",
    "\n",
    "chain = prompt | chat_model | StrOutputParser()\n",
    "\n",
    "chain.invoke({\n",
    "    \"question\": \"What is the current date?\",\n",
    "    \"current_date\": date.today()\n",
    "})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1920662d-9b26-47de-acd1-a3e67241854a",
   "metadata": {},
   "source": [
    ">**Note**: Debug verbosity is now deprecated in many core Runnables/chains—so it doesn’t always trigger output. It only on objects that support it, like AgentExecutor or when you pass verbose=True to a Runnable or agent—not on simple pipeline chains like prompt | model | parser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1ed9b50c-4665-49c8-87fa-d40cd57f6ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_verbose(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d2e23c7-fac0-4851-8697-ef4e798a5e61",
   "metadata": {},
   "source": [
    "### LangSmith Tracing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb1c789d-08e5-484d-8418-7e3c584731d2",
   "metadata": {},
   "source": [
    "You can enhance your LangChain applications by adding tracing with an external service like **LangSmith**—a powerful platform designed for debugging, evaluating, and monitoring LLM apps.\n",
    "\n",
    "**What is LangSmith?** \\\n",
    "LangSmith provides end-to-end observability for your language model workflows:\n",
    "- Tracks all inputs and outputs at every step\n",
    "- Helps you debug and test chains and agents effectively\n",
    "- Enables prompt engineering with version control and collaboration\n",
    "- Offers dashboards to monitor metrics like request rates, error rates, and costs\n",
    "- Supports automated evaluations and human-in-the-loop feedback\n",
    "\n",
    "**How to Add LangSmith Tracing?**\n",
    "1. [Sign up](https://smith.langchain.com/) and get your LangSmith API key from the LangSmith dashboard.\n",
    "2. Set environment variables to enable tracing and provide your API key:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "004a18e1-2a81-491e-9551-f0e4b1af85f5",
   "metadata": {},
   "source": [
    "After you sign up at the link above, make sure to set your environment variables to start logging traces:\n",
    "```\n",
    "export LANGSMITH_TRACING=\"true\"\n",
    "export LANGSMITH_API_KEY=\"...\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63ebade6-e3ee-4b59-95f3-95e534a1d60a",
   "metadata": {},
   "source": [
    "Or, if in a notebook, you can set them with:\n",
    "\n",
    "```python\n",
    "import getpass\n",
    "import os\n",
    "\n",
    "os.environ[\"LANGSMITH_TRACING\"] = \"true\"\n",
    "os.environ[\"LANGSMITH_API_KEY\"] = getpass.getpass()\n",
    "\n",
    "chain.invoke({\n",
    "  \"question\": \"What is the current date?\",\n",
    "  \"current_date\": date.today()\n",
    "}) \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeb099f9-9682-4712-929b-78a753554f68",
   "metadata": {},
   "source": [
    "### `astream_events()`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bb1f747-56dd-442b-a5de-33bd964ebc05",
   "metadata": {},
   "source": [
    "You can use the `astream_events()` method in LangChain to asynchronously receive detailed event data during the execution of your chains or models. This is especially useful when you want to integrate intermediate outputs or internal states directly into your application’s logic.\n",
    "\n",
    "Unlike simple streaming methods that yield only output tokens, `astream_events()` returns an `async` iterator over a variety of event objects, such as model start, intermediate token generation, and tool calls. This lets you selectively process or respond to specific event types as they occur in real-time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ab05ecf5-8abc-4b9a-a9ef-e8e787be298a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'event': 'on_chain_start', 'data': {'input': {'question': 'What is the current date?', 'current_date': datetime.date(2025, 7, 22)}}, 'name': 'RunnableSequence', 'tags': [], 'run_id': 'df18db9b-cd65-4d59-a28e-94e5e5051cee', 'metadata': {}, 'parent_ids': []}\n",
      "-----\n",
      "{'event': 'on_prompt_start', 'data': {'input': {'question': 'What is the current date?', 'current_date': datetime.date(2025, 7, 22)}}, 'name': 'ChatPromptTemplate', 'tags': ['seq:step:1'], 'run_id': 'a21a53cb-187f-4dc7-9583-d3035f983c98', 'metadata': {}, 'parent_ids': ['df18db9b-cd65-4d59-a28e-94e5e5051cee']}\n",
      "-----\n",
      "{'event': 'on_prompt_end', 'data': {'output': ChatPromptValue(messages=[SystemMessage(content='You know that the current date is \"2025-07-22\".', additional_kwargs={}, response_metadata={}), HumanMessage(content='What is the current date?', additional_kwargs={}, response_metadata={})]), 'input': {'question': 'What is the current date?', 'current_date': datetime.date(2025, 7, 22)}}, 'run_id': 'a21a53cb-187f-4dc7-9583-d3035f983c98', 'name': 'ChatPromptTemplate', 'tags': ['seq:step:1'], 'metadata': {}, 'parent_ids': ['df18db9b-cd65-4d59-a28e-94e5e5051cee']}\n",
      "-----\n",
      "{'event': 'on_chat_model_start', 'data': {'input': {'messages': [[SystemMessage(content='You know that the current date is \"2025-07-22\".', additional_kwargs={}, response_metadata={}), HumanMessage(content='What is the current date?', additional_kwargs={}, response_metadata={})]]}}, 'name': 'ChatGoogleGenerativeAI', 'tags': ['seq:step:2'], 'run_id': 'b5fcbc51-1892-4be4-a801-9c7fb40b2660', 'metadata': {'ls_provider': 'google_genai', 'ls_model_name': 'gemini-2.5-flash', 'ls_model_type': 'chat', 'ls_temperature': 0.0}, 'parent_ids': ['df18db9b-cd65-4d59-a28e-94e5e5051cee']}\n",
      "-----\n",
      "{'event': 'on_chat_model_stream', 'data': {'chunk': AIMessageChunk(content='The current date is 2025-07-22.', additional_kwargs={}, response_metadata={'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': []}, id='run--b5fcbc51-1892-4be4-a801-9c7fb40b2660', usage_metadata={'input_tokens': 26, 'output_tokens': 54, 'total_tokens': 80, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 38}})}, 'run_id': 'b5fcbc51-1892-4be4-a801-9c7fb40b2660', 'name': 'ChatGoogleGenerativeAI', 'tags': ['seq:step:2'], 'metadata': {'ls_provider': 'google_genai', 'ls_model_name': 'gemini-2.5-flash', 'ls_model_type': 'chat', 'ls_temperature': 0.0}, 'parent_ids': ['df18db9b-cd65-4d59-a28e-94e5e5051cee']}\n",
      "-----\n",
      "{'event': 'on_parser_start', 'data': {}, 'name': 'StrOutputParser', 'tags': ['seq:step:3'], 'run_id': '5e1efde8-5c18-4b5a-9504-196bf8621851', 'metadata': {}, 'parent_ids': ['df18db9b-cd65-4d59-a28e-94e5e5051cee']}\n",
      "-----\n",
      "{'event': 'on_parser_stream', 'run_id': '5e1efde8-5c18-4b5a-9504-196bf8621851', 'name': 'StrOutputParser', 'tags': ['seq:step:3'], 'metadata': {}, 'data': {'chunk': 'The current date is 2025-07-22.'}, 'parent_ids': ['df18db9b-cd65-4d59-a28e-94e5e5051cee']}\n",
      "-----\n",
      "{'event': 'on_chain_stream', 'run_id': 'df18db9b-cd65-4d59-a28e-94e5e5051cee', 'name': 'RunnableSequence', 'tags': [], 'metadata': {}, 'data': {'chunk': 'The current date is 2025-07-22.'}, 'parent_ids': []}\n",
      "-----\n",
      "{'event': 'on_chat_model_end', 'data': {'output': AIMessageChunk(content='The current date is 2025-07-22.', additional_kwargs={}, response_metadata={'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': []}, id='run--b5fcbc51-1892-4be4-a801-9c7fb40b2660', usage_metadata={'input_tokens': 26, 'output_tokens': 54, 'total_tokens': 80, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 38}}), 'input': {'messages': [[SystemMessage(content='You know that the current date is \"2025-07-22\".', additional_kwargs={}, response_metadata={}), HumanMessage(content='What is the current date?', additional_kwargs={}, response_metadata={})]]}}, 'run_id': 'b5fcbc51-1892-4be4-a801-9c7fb40b2660', 'name': 'ChatGoogleGenerativeAI', 'tags': ['seq:step:2'], 'metadata': {'ls_provider': 'google_genai', 'ls_model_name': 'gemini-2.5-flash', 'ls_model_type': 'chat', 'ls_temperature': 0.0}, 'parent_ids': ['df18db9b-cd65-4d59-a28e-94e5e5051cee']}\n",
      "-----\n",
      "{'event': 'on_parser_end', 'data': {'output': 'The current date is 2025-07-22.', 'input': AIMessageChunk(content='The current date is 2025-07-22.', additional_kwargs={}, response_metadata={'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': []}, id='run--b5fcbc51-1892-4be4-a801-9c7fb40b2660', usage_metadata={'input_tokens': 26, 'output_tokens': 54, 'total_tokens': 80, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 38}})}, 'run_id': '5e1efde8-5c18-4b5a-9504-196bf8621851', 'name': 'StrOutputParser', 'tags': ['seq:step:3'], 'metadata': {}, 'parent_ids': ['df18db9b-cd65-4d59-a28e-94e5e5051cee']}\n",
      "-----\n",
      "{'event': 'on_chain_end', 'data': {'output': 'The current date is 2025-07-22.'}, 'run_id': 'df18db9b-cd65-4d59-a28e-94e5e5051cee', 'name': 'RunnableSequence', 'tags': [], 'metadata': {}, 'parent_ids': []}\n",
      "-----\n"
     ]
    }
   ],
   "source": [
    "# Turn off debug mode for clarity\n",
    "set_debug(False)\n",
    "\n",
    "stream = chain.astream_events({\n",
    "    \"question\": \"What is the current date?\",\n",
    "    \"current_date\": date.today()\n",
    "})\n",
    "\n",
    "async for event in stream:\n",
    "    print(event)\n",
    "    print(\"-----\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe459fe1-8071-48b3-bf55-5403bba5d2e2",
   "metadata": {},
   "source": [
    "# Thank you!\n",
    "Congratulations on completing "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5ce5ce9-5bfa-48b5-b530-287e2d8d9371",
   "metadata": {},
   "source": [
    "### References \n",
    "- freecodecamp - [How to Use LangChain to Build With LLMs – A Beginner's Guide](https://www.freecodecamp.org/news/beginners-guide-to-langchain/)\n",
    "- LangChain - [How-to guides](https://python.langchain.com/docs/how_to/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
